"""
sample.py

Module containing functions related to sampling from a trained model
"""

from math import exp, isfinite, ceil

import numpy as np
import torch

from reportengine import collect


def sample_batch(
    loaded_model, action, batch_size, state_i=None):
    r"""
    Sample using Metroplis-Hastings algorithm from a large number of phi
    configurations.

    We calculate the condition

        A = min[1, (\tilde p(phi^i) * p(phi^j)) / (p(phi^i) * \tilde p(phi^j))]

    Where i is the index of the current phi in metropolise chain and j is the
    current proposal. A uniform random number, u, is drawn and if u <= A then
    the proposed state phi^j is accepted (and becomes phi^i for the next update)

    Parameters
    ----------
    loaded_model: Module
        loaded_model which is going to be used to generate sample states
    action: Module
        the action upon which the loaded_model was trained, used to calculate the
        acceptance condition
    batch_size: int
        the number of states to generate from the loaded_model
    state_i: torch.Tensor or None
        the current state of the chain. None if this is the first batch

    Returns
    -------
    phi[chain_indices, :]: torch.Tensor
        chain of configurations generated by the MH algorithm
    history: torch.BoolTensor
        boolean tensor containing accept/reject history of chain
    state: torch.Tensor
        the current phi state, for continuity between batches
    """
    with torch.no_grad():  # don't track gradients
        z = torch.randn((batch_size, loaded_model.size_in))  # random z configurations
        phi = loaded_model.inverse_map(z)  # map using trained loaded_model to phi
        if state_i is not None:
            phi[0] = state_i
        log_ptilde = loaded_model(phi)
    history = torch.zeros(batch_size, dtype=torch.bool)  # accept/reject history
    chain_indices = torch.zeros(batch_size, dtype=torch.long)

    log_ratio = log_ptilde + action(phi)
    if not isfinite(exp(float(min(log_ratio) - max(log_ratio)))):
        raise ValueError("could run into nans")

    i = 0
    log_ratio_i = log_ratio[0]
    for j in range(1, batch_size):
        condition = min(1, exp(float(log_ratio_i - log_ratio[j])))
        if np.random.uniform() <= condition:
            chain_indices[j] = j
            log_ratio_i = log_ratio[j]
            state_i = phi[j]
            i = j
            history[j] = True  # accepted -> True
        else:
            chain_indices[j] = i

    return phi[chain_indices, :], history, state_i


def thermalisation(loaded_model, action):
    r"""
    A (hopefully) short initial sampling phase to allow the system to thermalise.

    Parameters
    ----------
    loaded_model: Module
        loaded_model which is going to be used to generate sample states
    action: Module
        the action upon which the loaded_model was trained, used to calculate the
        acceptance condition

    Returns
    -------
    state: torch.Tensor
        the current phi state, for continuity between batches
    log_ratio: float
        the ratio associated with state
    """
    t_therm = 1000 # ideally come up with a way of working this out on the fly

    state = sample_batch(
            loaded_model, action, t_therm)[2]

    print(f"Thermalisation: discarded {t_therm} configurations.")
    return state


def chain_autocorrelation(loaded_model, action, state) -> float:
    r"""
    Compute an observable-independent measure of the integrated autocorrelation
    time for the Markov chain.

        \tau_int = 0.5 + sum_{\tau=1}^{\tau_max} \rho(\tau)/\rho(0)

    where \rho(\tau)/\rho(0) is the probability of \tau consecutive rejections,
    which we estimate by

        \rho(\tau)/\rho(0) = # consecutive runs of \tau rejections / (N - \tau)

    See eqs. (16) and (19) in https://arxiv.org/pdf/1904.12072.pdf

    This measure of autocorrelation is used to provide a first guess for an
    appropriate subsampling interval,

        tskip = ceil(2 * integrated_autocorrelation)

    with the intended effect being that observables on the subsampled chain
    are entirely decorrelated.

    See http://luscher.web.cern.ch/luscher/lectures/LesHouches09.pdf section 2.2.4

    Parameters
    ----------
    loaded_model: Module
        loaded_model which is going to be used to generate sample states
    action: Module
        the action upon which the loaded_model was trained, used to calculate the
        acceptance condition

    Returns
    -------
    state: torch.Tensor
        the current phi state, for continuity between batches
    log_ratio: float
        the ratio associated with state
    tskip: float
        Guess for subsampling interval, based on the integrated autocorrelation time

    """
    batch_size = 1000 # Hard coded num states for estimating integrated autocorrelation

    # Sample some states
    history, state = sample_batch(
            loaded_model, action, batch_size, state
    )[1:]

    N = len(history)
    autocorrelations = torch.zeros(N - 1, dtype=torch.float)
    consecutive_rejections = 0

    for step in history:
        if step == True:  # move accepted
            if consecutive_rejections > 0:  # faster than unnecessarily accessing array
                autocorrelations[1 : consecutive_rejections + 1] += 1
            consecutive_rejections = 0
        else:  # move rejected
            consecutive_rejections += 1
    if consecutive_rejections > 0:  # pick up last rejection run
        autocorrelations[1 : consecutive_rejections + 1] += 1

    # Compute integrated autocorrelation
    integrated_autocorrelation = 0.5 + torch.sum(
        autocorrelations / (N - torch.arange(N - 1))
    )

    tskip = ceil(2 * integrated_autocorrelation)
    print(
        f"Guess for sampling interval: {tskip}, based on {batch_size} configurations."
    )

    return state, tskip


def sample(loaded_model, action, target_length: int) -> torch.Tensor:
    r"""
    Produces a Markov chain with approximately target_length decorrelated configurations,
    using the Metropolis-Hastings algorithm.

    Parameters
    ----------
    loaded_model: Module
        loaded_model which is going to be used to generate sample states
    action: Module
        the action upon which the loaded_model was trained, used to calculate the
        acceptance condition
    target_length: int
        the desired number of states to generate from the loaded_model

    Returns
    -------
    sample: torch.Tensor
        a sample of states from loaded_model, size = (target_length, loaded_model.size_in)

    """

    # Thermalise
    state = thermalisation(loaded_model, action)

    # Estimate observable-independent autocorrelation time
    state, tskip = chain_autocorrelation(
            loaded_model, action, state
    )

    # Decide how many configurations to generate, in order to get approximately
    # target_length after picking out decorrelated configurations
    batch_size = min(target_length, 10000)  # hard coded for now
    n_large = target_length * tskip
    Nbatches = ceil(n_large / batch_size)
    n_large = Nbatches * batch_size

    full_chain = torch.empty((n_large, loaded_model.size_in), dtype=torch.float32)
    history = torch.empty(n_large, dtype=torch.bool)  # accept/reject history

    for batch in range(Nbatches):
        # Generate sub-chain of batch_size configurations
        batch_chain, batch_history, state = sample_batch(
            loaded_model, action, batch_size, state
        )

        # Add to larger chain
        start = batch * batch_size
        full_chain[start : start + batch_size, :] = batch_chain
        history[start : start + batch_size] = batch_history

    # Accept-reject statistics
    accepted = torch.sum(history)
    rejected = n_large - accepted
    fraction = accepted / float(accepted + rejected)
    print(f"Accepted: {accepted}, Rejected: {rejected}, Fraction: " f"{fraction:.2g}")

    # Pick out every tskip configuration
    decorrelated_chain = full_chain[::tskip]

    print(
        f"Produced a chain of length: {n_large}, but returning a "
        f"decorrelated chain of length: {len(decorrelated_chain)}"
    )

    return decorrelated_chain


sample_training_output = collect("sample", ("training_context",))
